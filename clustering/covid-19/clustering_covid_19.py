# -*- coding: utf-8 -*-
"""clustering-covid-19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_M-Ja5caiPTUdjDLvOQG41ysMmN5LYKH

# 1. Source Data

1. Sumber dataset : Kaggle.com
2. Link dataset : https://www.kaggle.com/datasets/hendratno/covid19-indonesia
3. Penjelasan dataset : Dataset ini merupakan dataset Covid-19 di Indonesia untuk membantu dalam mengambil sebuah keputusan terkait covid-19.

# 2. Load Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import math
import pandas as pd
from sklearn.preprocessing import RobustScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans

"""# 3. Load Data dan Eksplorasi Data"""

data_covid = pd.read_csv(
    "https://raw.githubusercontent.com/asepsr37/data-and-machine-learning/main/clustering/covid-19/resource/covid_19_indonesia_time_series_all.csv"
)

# Mengetahui Jumlah baris dan kolom
jumlah_baris, jumlah_kolom = data_covid.shape
print("Jumlah baris adalah ", jumlah_baris)
print("Jumlah kolom adalah ", jumlah_kolom)

data_covid.columns

# Mengetahui tipe data
data_covid.info()

# Mengetahui Missing Value
data_covid.isnull().sum()

# Mengetahui data yang duplikat
data_covid.duplicated().sum()

# Melihat persebaran data
num_cols_numeric = len(kolom_numerik)
num_rows_numeric = math.ceil(num_cols_numeric / 3)

plt.figure(figsize=(12, 6 * num_rows_numeric))

for i, column in enumerate(kolom_numerik):
    plt.subplot(num_rows_numeric, 3, i + 1)
    sns.histplot(data=data_covid, x=column, kde=True)

plt.tight_layout()
plt.show()

def plot_detailed_outliers_with_nan(df):
    # 1. Pilih hanya kolom numerik
    numeric_cols = df.select_dtypes(include=[np.number]).columns

    n_cols = 3
    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5 * n_rows))
    axes = axes.flatten()

    outlier_summary = []

    for i, col in enumerate(numeric_cols):
        # Ambil data non-null hanya untuk perhitungan (tanpa menghapus di dataframe asli)
        clean_series = df[col].dropna()

        if clean_series.empty:
            continue

        # Hitung statistik IQR pada data yang tersedia
        Q1 = clean_series.quantile(0.25)
        Q3 = clean_series.quantile(0.75)
        IQR = Q3 - Q1

        mild_upper = Q3 + (1.5 * IQR)
        mild_lower = Q1 - (1.5 * IQR)
        ext_upper = Q3 + (3.0 * IQR)
        ext_lower = Q1 - (3.0 * IQR)

        # Hitung jumlah berdasarkan data non-null
        count_mild = clean_series[(clean_series > mild_upper) & (clean_series <= ext_upper) |
                                  (clean_series < mild_lower) & (clean_series >= ext_lower)].shape[0]
        count_ext = clean_series[(clean_series > ext_upper) | (clean_series < ext_lower)].shape[0]

        # Simpan summary dengan pembagi total data non-null agar persentase akurat
        outlier_summary.append({
            'Feature': col,
            'Missing Values': df[col].isna().sum(),
            'Mild Outliers': count_mild,
            'Extreme Outliers': count_ext,
            '% Extreme (of non-null)': round((count_ext / len(clean_series)) * 100, 2)
        })

        # Visualisasi (Seaborn otomatis mengabaikan NaN)
        sns.boxplot(x=df[col], ax=axes[i], color='skyblue', flierprops={"marker": "o", "markersize": 4, "alpha": 0.5})

        axes[i].axvline(mild_upper, color='orange', linestyle='--', label='Mild Fence' if i==0 else "")
        axes[i].axvline(ext_upper, color='red', linestyle='-', label='Extreme Fence' if i==0 else "")

        axes[i].set_title(f"{col}\nNaN: {df[col].isna().sum()} | Ext: {count_ext}", fontsize=12)
        axes[i].set_xlabel("")

    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()

    return pd.DataFrame(outlier_summary)

# Penggunaan
summary_df = plot_detailed_outliers_with_nan(data_covid)
summary_df



# Melihat distribusi data pada kolom bertipe data kategorikal
categorical_columns = ['Location', 'Province', 'Island']
plt.figure(figsize=(10, len(categorical_columns) * 4))
for i, col in enumerate(categorical_columns, 1):
    plt.subplot(len(categorical_columns), 1, i)
    sns.countplot(y=data_covid[col], order=data_covid[col].value_counts().index, palette='viridis')
    plt.title(f'Distribusi Kategori: {col}')
    plt.xlabel('Jumlah')

plt.tight_layout()
plt.show()

kolom_numerik = data_covid.select_dtypes(include=['number'])
plt.figure(figsize=(16, 6))
sns.heatmap(kolom_numerik.corr(), annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5)
plt.title("Heatmap Korelasi pada tipe data numerik")
plt.show()

"""# 4. Membersihkan Data"""

# Menghapus Kolom yang tidak perlu digunakan
kolom_dihapus = [
    'Date', 'Location ISO Code', 'Location', 'Location Level', 'Country',
    'Continent', 'Time Zone', 'Special Status', 'City or Regency',
    'Total Regencies', 'Total Cities', 'Total Districts',
    'Total Urban Villages', 'Total Rural Villages', 'Area (km2)',
    'Longitude', 'Latitude','Population Density',
    'Total Deaths per 100rb'
]
data_covid = data_covid.drop(columns=kolom_dihapus)

"""1. Identitas & Metadata (Hapus dari Model):
- Date, Location ISO Code, Location, Location Level, Country, Continent, Time Zone, Special Status.
- Alasan: Ini adalah data administratif/teks yang tidak bisa dihitung jaraknya oleh algoritma K-Means atau PCA.

2. Informasi Geografis Statis (Hapus):
- City or Regency, Total Regencies, Total Cities, Total Districts, Total Urban Villages, Total Rural Villages, Area (km2), Longitude, Latitude.
- Alasan: Kolom-kolom ini menjelaskan struktur wilayah, bukan tingkat keparahan virus secara langsung.

3. Redundansi Data Absolut (Hapus):
- Population Density.
- Alasan: Data absolut sangat dipengaruhi oleh jumlah penduduk (Provinsi besar pasti memiliki kasus lebih banyak). Untuk pemetaan tingkat risiko yang adil, kita lebih membutuhkan data Relatif (seperti "per Million"). Selain itu, Population sudah terwakili dalam perhitungan fitur "per Million".

4. Redundansi Skala (Hapus):
- Total Deaths per 100rb.
- Alasan: Ini redundan dengan Total Deaths per Million (hanya beda skala faktor 10).
"""

# Menghapus Missing Value
data_covid = data_covid.dropna()

# Menghapus duplikat
data_covid = data_covid.drop_duplicates()

# Ubah nilai data menjadi float
kolom_rate = ['Case Fatality Rate', 'Case Recovered Rate']

for col in kolom_rate:
    data_covid[col] = (
        data_covid[col]
        .str.replace('%', '', regex=False)
        .str.replace(',', '.', regex=False)
    )
    data_covid[col] = pd.to_numeric(data_covid[col], errors='coerce')

# Ganti nama kolom, tambahkan (%)
data_covid.rename(columns={
    'Case Fatality Rate': 'Case Fatality Rate (%)',
    'Case Recovered Rate': 'Case Recovered Rate (%)'
}, inplace=True)

def apply_log_transformation(df):
    """
    1. Mengidentifikasi kolom numerik
    2. Menerapkan Log Transformation (log1p) pada kolom yang valid.
    """
    # 1. Identifikasi kolom numerik
    numeric_cols = df.select_dtypes(include=[np.number]).columns

    # 2. Buat copy dataframe agar data asli tidak berubah
    df_log = df.copy()

    # 3. Proses Transformasi Log
    for col in numeric_cols:
        # Cek apakah kolom memiliki nilai negatif (Log tidak bisa untuk nilai <= -1)
        if (df_log[col] < 0).any():
            print(f"Peringatan: Kolom '{col}' mengandung nilai negatif. Log transformasi dilewati.")
            continue

        # 4. Terapkan log(x+1)
        # np.log1p(NaN) akan tetap menghasilkan NaN secara otomatis
        df_log[col] = np.log1p(df_log[col])

    return df_log

# apply
data_covid_log = apply_log_transformation(data_covid)

# Cek hasil
print(f"Ukuran dataset (baris tetap sama): {data_covid_log.shape}")
display(data_covid_log.head())

# Melihat persebaran data
kolom_numerik = data_covid_log.select_dtypes(include=[np.number]).columns
num_cols_numeric = len(kolom_numerik)
num_rows_numeric = math.ceil(num_cols_numeric / 3)

plt.figure(figsize=(12, 6 * num_rows_numeric))

for i, column in enumerate(kolom_numerik):
    plt.subplot(num_rows_numeric, 3, i + 1)
    sns.histplot(data=data_covid_log, x=column, kde=True)

plt.tight_layout()
plt.show()

def plot_detailed_outliers_with_nan(df):
    # 1. Pilih hanya kolom numerik
    numeric_cols = df.select_dtypes(include=[np.number]).columns

    n_cols = 3
    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5 * n_rows))
    axes = axes.flatten()

    outlier_summary = []

    for i, col in enumerate(numeric_cols):
        # Ambil data non-null hanya untuk perhitungan (tanpa menghapus di dataframe asli)
        clean_series = df[col].dropna()

        if clean_series.empty:
            continue

        # Hitung statistik IQR pada data yang tersedia
        Q1 = clean_series.quantile(0.25)
        Q3 = clean_series.quantile(0.75)
        IQR = Q3 - Q1

        mild_upper = Q3 + (1.5 * IQR)
        mild_lower = Q1 - (1.5 * IQR)
        ext_upper = Q3 + (3.0 * IQR)
        ext_lower = Q1 - (3.0 * IQR)

        # Hitung jumlah berdasarkan data non-null
        count_mild = clean_series[(clean_series > mild_upper) & (clean_series <= ext_upper) |
                                  (clean_series < mild_lower) & (clean_series >= ext_lower)].shape[0]
        count_ext = clean_series[(clean_series > ext_upper) | (clean_series < ext_lower)].shape[0]

        # Simpan summary dengan pembagi total data non-null agar persentase akurat
        outlier_summary.append({
            'Feature': col,
            'Missing Values': df[col].isna().sum(),
            'Mild Outliers': count_mild,
            'Extreme Outliers': count_ext,
            '% Extreme (of non-null)': round((count_ext / len(clean_series)) * 100, 2)
        })

        # Visualisasi (Seaborn otomatis mengabaikan NaN)
        sns.boxplot(x=df[col], ax=axes[i], color='skyblue', flierprops={"marker": "o", "markersize": 4, "alpha": 0.5})

        axes[i].axvline(mild_upper, color='orange', linestyle='--', label='Mild Fence' if i==0 else "")
        axes[i].axvline(ext_upper, color='red', linestyle='-', label='Extreme Fence' if i==0 else "")

        axes[i].set_title(f"{col}\nNaN: {df[col].isna().sum()} | Ext: {count_ext}", fontsize=12)
        axes[i].set_xlabel("")

    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()

    return pd.DataFrame(outlier_summary)

# Penggunaan
summary_df = plot_detailed_outliers_with_nan(data_covid_log)
print(summary_df)

"""Dalam  menangani outliers menggunakan log transformation, kondisi distrubsi data cenderung normal walauppun dibeberapa data tidak normal. Lalu, setelah dicek menggunakan boxplot di beberapa kolom seperti pada New Actives Cases dan total active cases mmemiliki outliers yang tinggi juga. Namun, akan dilakukan robust scaling untuk menangani outliers tersebut"""

data_covid_log

# 1. Memisahkan fitur kategorikal (Province & Island)
#  karena Fitur ini tidak masuk ke PCA/Clustering
df_metadata = data_covid_log[['Province', 'Island']].copy()

# 2. Ambil hanya kolom numerik untuk pemodelan
df_numeric = data_covid_log.select_dtypes(include=['number']).copy()

# 3. Inisialisasi RobustScaler
scaler = RobustScaler()

# 4. Fit dan Transform pada data numerik
# RobustScaler akan menghitung median dan IQR untuk setiap kolom
scaled_data = scaler.fit_transform(df_numeric)

# 5. Kembalikan ke format DataFrame agar nama kolom tetap terjaga
df_scaled = pd.DataFrame(scaled_data, columns=df_numeric.columns)

df_scaled

"""# 5. Modelling

"""

# Selection Feature menggunakan PCA
pca = PCA()
pca.fit(df_scaled)

explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance_ratio)
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')
plt.xlabel('Jumlah Komponen Utama')
plt.ylabel('Variansi Kumulatif')
plt.title('Menentukan Jumlah Komponen PCA')
plt.grid()
plt.show()

"""PCA (Principal Component Analysis) adalah teknik statistik untuk reduksi dimensi data. Tujuannya adalah:

- Menyederhanakan data kompleks - mengubah banyak variabel yang saling berkorelasi menjadi variabel baru (komponen utama) yang tidak berkorelasi
- Menangkap variasi maksimal - komponen pertama menangkap variasi terbesar, komponen kedua variasi terbesar berikutnya, dan seterusnya
- Mengurangi dimensi - dari 15 kolom bisa diringkas menjadi lebih sedikit tanpa kehilangan banyak informasi

- Sumbu X: Jumlah komponen utama (dari 1 hingga 15)
- Sumbu Y: Variansi kumulatif (proporsi total variasi data yang dijelaskan)

Analisis Kurva:

- Sumbu X: Jumlah komponen utama (dari 1 hingga 15)
- Sumbu Y: Variansi kumulatif (proporsi total variasi data yang dijelaskan)

1. Komponen 1-5: Kurva naik tajam

    - 5 komponen pertama sudah menjelaskan ~75-80% variasi data


2. Komponen 6-8: Kurva mulai landai

    - 8 komponen menjelaskan ~90% variasi


3. Komponen 9-15: Kurva hampir datar

    - Komponen tambahan hanya menambah sedikit informasi

Rekomendasi:
Berdasarkan grafik, maka ggunakan 5-8 komponen utama karena:

- Menangkap 80-90% informasi dari data asli
- Mengurangi dimensi dari 15 menjadi 5-8 (reduksi ~50-70%)
- Titik "elbow" (siku) berada di sekitar komponen 5-8
"""

def run_pca_analysis(df_input, n_pca):
    """
    Input: Dataframe hasil scaling, jumlah komponen PCA
    Output: Dataframe hasil PCA (siap untuk clustering)
    """
    # Tahap PCA
    pca = PCA(n_components=n_pca)
    pca_data = pca.fit_transform(df_input)
    pca_df = pd.DataFrame(pca_data, columns=[f"PC{i+1}" for i in range(n_pca)])

    # Analisis Silhouette untuk mencari k-optimal
    scores = []
    k_range = range(2, 11)
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(pca_df)
        scores.append(silhouette_score(pca_df, labels))

    # Visualisasi
    plt.figure(figsize=(8, 4))
    plt.plot(k_range, scores, marker='o', linestyle='--', color='teal')
    plt.title(f'Analisis Silhouette Score (PCA: {n_pca})')
    plt.xlabel('Jumlah Cluster (k)')
    plt.ylabel('Score')
    plt.grid(True)
    plt.show()

    return pca_df

def run_final_clustering(pca_df, k_final):
    """
    Input: Dataframe hasil PCA, jumlah cluster pilihan
    Output: Dataframe dengan kolom cluster & nilai silhouette score
    """
    # Copy agar data PCA asli tidak berubah
    df_result = pca_df.copy()

    # Pemodelan K-Means
    kmeans = KMeans(n_clusters=k_final, random_state=42, n_init=10)
    df_result['Cluster'] = kmeans.fit_predict(df_result)

    # Hitung Score (hanya pada fitur PC, kolom Cluster tidak dihitung)
    score = silhouette_score(pca_df, df_result['Cluster'])

    print(f"--- Evaluasi Model Akhir ---")
    print(f"Jumlah Cluster: {k_final}")
    print(f"Silhouette Score: {score:.4f}\n")

    return df_result, score

"""## PCA 5"""

pca_df_5 = run_pca_analysis(df_scaled, n_pca=5)

# grafik k=2 terlihat paling optimal
final_model_5_2, score_5_2 = run_final_clustering(pca_df_5, k_final=2)

# mencoba k=3
final_model_5_3, score_5_3 = run_final_clustering(pca_df_5, k_final=3)



"""## PCA 6"""

pca_df_6 = run_pca_analysis(df_scaled, n_pca=6)

# grafik k=2 terlihat paling optimal
final_model_6_2, score_6_2 = run_final_clustering(pca_df_6, k_final=2)

# mencoba k=3
final_model_6_3, score_6_3 = run_final_clustering(pca_df_6, k_final=3)

"""## PCA 7"""

pca_df_7 = run_pca_analysis(df_scaled, n_pca=7)

# grafik k=2 terlihat paling optimal
final_model_7_2, score_7_2 = run_final_clustering(pca_df_7, k_final=2)

# mencoba k=3
final_model_7_3, score_7_3 = run_final_clustering(pca_df_7, k_final=3)

"""## PCA 8"""

pca_df_8 = run_pca_analysis(df_scaled, n_pca=8)

# grafik k=2 terlihat paling optimal
final_model_8_2, score_8_2 = run_final_clustering(pca_df_8, k_final=2)

# mencoba k=3
final_model_8_3, score_8_3 = run_final_clustering(pca_df_8, k_final=3)



"""# 6. Evaluasi"""

# Menyusun data dari setiap score
data_skenario = {
    'PCA Components': [5, 5, 6, 6, 7, 7, 8, 8],
    'Clusters (k)': [2, 3, 2, 3, 2, 3, 2, 3],
    'Silhouette Score': [
        score_5_2, score_5_3,
        score_6_2, score_6_3,
        score_7_2, score_7_3,
        score_8_2, score_8_3
    ]
}

# Membuat DataFrame perbandingan
df_perbandingan = pd.DataFrame(data_skenario)

# Menampilkan tabel yang diurutkan dari skor tertinggi
df_perbandingan_sorted = df_perbandingan.sort_values(by='Silhouette Score', ascending=False)

print("Tabel Hasil Perbandingan Skenario Model (KP2):")
display(df_perbandingan_sorted)

# Pastikan data sudah diurutkan
df_plot = df_perbandingan_sorted.copy()

# Buat label skenario
df_plot['Skenario'] = (
    'PCA ' + df_plot['PCA Components'].astype(str) +
    ' | k=' + df_plot['Clusters (k)'].astype(str)
)

# Plot bar chart
plt.figure(figsize=(12, 6))
plt.bar(df_plot['Skenario'], df_plot['Silhouette Score'])
plt.ylim(0.9, 1.0)
plt.xticks(rotation=45, ha='right')
plt.xlabel('Skenario PCA dan Jumlah Cluster')
plt.ylabel('Silhouette Score')
plt.title('Perbandingan Silhouette Score pada Berbagai Skenario Clustering')

plt.tight_layout()
plt.show()

"""# 7. Implementasi Model Terbaik"""

# 1. Mengambil hanya kolom 'Cluster' dari hasil model PCA 5
# gunakan double bracket [['Cluster']] agar tetap dalam format DataFrame
df_labels = final_model_5_2[['Cluster']]

# 2. Menyatukan dengan df_scaled (data numerik hasil scaling)
# Gunakan axis=1 untuk menggabungkan secara horizontal (menambah kolom)
# Pastikan index keduanya masih sama (tidak di-reset secara berbeda sebelumnya)
df_scaled_with_cluster = pd.concat([df_scaled, df_labels], axis=1)

# 3. Menampilkan hasil penggabungan
print("Dataframe numerik terskala dengan label Cluster:")
display(df_scaled_with_cluster.head())

# 1. Identifikasi kolom mana saja yang benar-benar di-Log sebelumnya
# Kita ambil kolom numerik yang TIDAK memiliki nilai negatif dari data awal
cols_logged = [col for col in data_covid.select_dtypes(include=[np.number]).columns
               if not (data_covid[col] < 0).any()]

# A. Pisahkan kolom Cluster
clusters = df_scaled_with_cluster['Cluster']
df_only_scaled = df_scaled_with_cluster.drop('Cluster', axis=1)

# B. Langkah 1: Inverse Robust Scaling (Untuk semua kolom)
data_inv_scaled = scaler.inverse_transform(df_only_scaled)
df_inv_scaled = pd.DataFrame(data_inv_scaled, columns=df_only_scaled.columns)

# C. Langkah 2: Inverse Log Transformation SELEKTIF
df_original_scale = df_inv_scaled.copy()

for col in df_original_scale.columns:
    if col in cols_logged:
        # Hanya kolom yang di-log yang dibalikkan dengan expm1
        df_original_scale[col] = np.expm1(df_original_scale[col])
    else:
        # Kolom yang mengandung nilai negatif (seperti Active Cases) dibiarkan apa adanya
        # karena sebelumnya memang tidak di-log
        df_original_scale[col] = df_original_scale[col]

# D. Gabungkan kembali dengan Metadata dan Cluster
df_final_original = pd.concat([
    df_metadata.reset_index(drop=True),
    df_original_scale,
    clusters.reset_index(drop=True)
], axis=1)

# E. Konversi ke Integer (Nullable) agar angka 2.0 menjadi 2
# Tangani juga jika ada nilai infinity (inf) akibat perhitungan matematik
cols_to_fix = df_original_scale.columns
df_final_original[cols_to_fix] = df_final_original[cols_to_fix].replace([np.inf, -np.inf], np.nan)
df_final_original[cols_to_fix] = df_final_original[cols_to_fix].round().astype('Int64')

display(df_final_original.head())

"""# 8. Profilling

"""

covid_cluster = pd.read_csv("covid_cluster.csv")

# Menghitung rata-rata kasus asli untuk setiap Cluster
cluster_profile = covid_cluster.groupby('Cluster').mean(numeric_only=True)

# Menghitung jumlah provinsi (count) yang masuk ke setiap cluster
cluster_profile['Jumlah_Provinsi'] = covid_cluster.groupby('Cluster')['Province'].count()

# Menampilkan tabel profiling (diurutkan berdasarkan kasus tertinggi)
print("--- Profil Karakteristik Zona Risiko (Nilai Asli) ---")
display(cluster_profile.sort_values(by='Total Cases', ascending=False))

print(cluster_profile)

"""# SAVE"""

# df_final_original.to_csv('covid_cluster.csv', index=False)